\section{Discussion}
\label{sec:discussion}

In this work, we have studied how joint regularization of all regression coefficients in Bayesian linear multilevel models can be carried out. To do so, we have have proposed the R2D2M2 prior that imposes a prior on the global coefficient of determination $R^2$ and propagates the resulting regularization to  the individual regression coefficients via a Dirichlet Decomposition. The chosen parameterization of the R2D2M2 prior offers intuitive and interpretable hyperparameters that are easy to understand in practice and offers analysts the ability to readily incorporate prior beliefs into their multilevel model. Together with the fact that only very few hyperparameters have to be specified, this greatly simplifies prior specification for these models.

We have also derived shrinkage factors and demonstrated how to compute the effective number of non-zero coefficients of the model. This allows the analyst to have a direct understanding of the implications that the chosen hyperparameters have on the amount of shrinkage imposed by the prior.  We have shown, theoretically and via simulations, that the R2D2M2 prior enables both local and global shrinkage of the regression coefficients and that it possesses vital properties that are required when working on high-dimensional regression problems, such as sufficient concentration of mass near the origin as well as heavy tails.  Finally we have demonstrated, via intensive simulations and by analysing real life data, that our prior implementation is not only well calibrated, but also offers reliable estimation even in the most complex cases that we investigated.

Previously, established priors for regression coefficients in the context of multilevel modelling have only been able to either jointly regularize the overall coefficients or the varying coefficients but never both at the same time. In the case of the latter, it has been done either in the standard way of using multivariate normal distribution with fixed hyperparameters \citep{gelman_hill_2006} or by constructing a joint prior over their variance parameters \citep{Fulgstad2019}. Additionally, dealing with high dimensionality in the multilevel context has been fairly restrictive, since it has been concerned with the scenario of a larger number of overall coefficients, i.e., $p >N$, while including only very few varying terms, i.e $q \ll N$. The R2D2M2 prior allows for a joint regularization of both the overall and varying coefficients and provides the opportunity to study high dimensional scenarios in which both $p >N$ and $q>N$.

Even though it is natural to wonder how the R2D2M2 prior model compares to frequentist methods to analyze multilevel models, we stress that we are focused on very high dimensional scenarios ($p,q>N$), where we need to regularize strongly due to the complexity of the problem. Existing frequentist methods and corresponding implementations for multilevel models that we are aware of are however not able to handle a high number of varying terms ($q>N$) and also struggle with a high number of overall terms ($p > N$) due to the resulting rank-deficient design matrices. Accordingly, comparing our proposed Bayesian model with frequentist methods would not be sensible at this point.

The present work can be extended in several ways. For instance, we could consider studying regularized versions of the R2D2M2 prior as \cite{PiironenHorseshoe} did for the original Horseshoe prior \citep{Horseshoe}. We have shown in simulations that as the complexity of the problem increases in terms of number of coefficients included the model, the prior reacts by increasing the amount of exerted shrinkage. However, coefficients that have a sufficiently high magnitudes will basically remain unaltered. Even though this is considered a key strength of shrinkage priors by some accounts \citep{BayesPenalizedRegSara}, it may not always be desirable and can be harmful when the parameters are only weakly identified by the data \citep{PiironenHorseshoe}.  Alternatively, deriving joint priors for non-normal likelihoods is an important area for future research. For instance \cite{bayesianworkflow} demonstrate for logistic regression that, as the number of covariates increases, quasi-complete separability becomes more likely, thus reducing the stability of the model if no substantial regularization is imposed.

Prior specification is a vital step in the Bayesian Workflow \citep{bayesianworkflow} and directly influences the performance of the models under consideration. Specifying joint priors allows us to account for the increasing complexity implied by increasing the amount of terms and parameters in the model, for example, in the shape of multilevel structure. What is more, they allow us to avoid some of the undesirable consequences of using independent, weakly informative priors in high dimensional setting. Ideally, joint priors should be able to express the user's prior knowledge while at the same time being able to perform efficient regularization, both globally and locally. These ideas together with the new developments presented here lay out a new avenue of research in the field of prior specification and elicitation.
