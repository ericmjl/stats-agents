%% BiBTeX database for models.tex bibliography
%%
%% bibtexfile{
%%  author      = "James W. Hardin",
%%  version     = "1.00",
%%  date        = "14 March 1999",
%%  filename    = "models.bib",
%%  address     = "Department of Statistics
%%                 Texas A\&M University
%%                 Mail Stop 3143
%%                 College Station, TX
%%                 77843-3143
%%                 USA",
%%  email       = "<jhardin at stat.tamu.edu>" }

@String{JASA  = {{Journal of the American Statistical Association}}}
@String{JCGS  = {{Journal of Computational and Graphical Statistics}}}
@String{JRSSA = {{Journal of the Royal Statistical Society - Series A}}}
@String{JRSSB = {{Journal of the Royal Statistical Society - Series B}}}

***** BEGIN jsp added
@Inproceedings{test,
 author         = {J. S. David},
 title          = {sts14: Bivariate Granger causality test. {\em Bayesian Analysis Technical Bulletin} 51:  40--41},
 booktitle      = {Bayesian Analysis Technical Bulletin Reprints},
 publisher      = {Bayesian Analysis Press},
 volume         = {9},
 address        = {College Station, TX},
 pages          = {350--351},
 year           = {1999}
}

@Book{graphicx,
  author	= {Goossens, M. and Mittelbach, F. and Rahtz, A. and Roegel, D. and Vo\ss, H.},
  title 	= {The \LaTeX{} Graphics Companion},
  publisher	= {Addison--Wesley},
  address	= {Reading, MA},
  edition	= {Second},
  year  	= {2008}
}

@Book{companion,
  author	= {Goossens, M. and Mittelbach, F. and Samarin, A.},
  title 	= {The \LaTeX{} Companion},
  publisher	= {Addison--Wesley},
  address	= {Reading, MA},
  edition	= {},
  year  	= {1994}
}



@Book{webcompanion,
  author	= {Goossens, M. and Rahtz, S.},
  title 	= {The \LaTeX{} Web Companion},
  publisher	= {Addison--Wesley},
  address	= {Reading, MA},
  edition	= {},
  year  	= {1999}
}

@Book{latexbook,
  author	= {L. Lamport},
  title 	= {\LaTeX: a document prapration system},
  publisher	= {Addison--Wesley},
  address	= {Reading, MA},
  edition	= {},
  year  	= {1994}
}

@Book{texbook,
  author	= {D.~E. Knuth},
  title 	= {The \TeX{} book},
  publisher	= {Addison--Wesley},
  address	= {Reading, MA},
  edition	= {},
  year  	= {1986}
}

@Book{davisonhinkley,
  author	= {A.~C. Davison and D.~V. Hinkley},
  title 	= {Bootstrap Methods and their Application},
  publisher	= {Campridge University Press},
  address	= {Cambridge},
  edition	= {},
  year  	= {1997}
}

@Book{efrontibshirani,
  author	= {B. Efron and R.~J. Tibshirani},
  title 	= {An Introduction to the Bootstrap},
  publisher	= {Chapman \& Hall},
  address	= {New York},
  edition	= {},
  year  	= {1993}
}

@Book{mooneyduval,
  author	= {C.~Z. Mooney and R.~D. Duval},
  title 	= {Bootstrapping: A Nonparametric Approach to Statistical Inference},
  publisher	= {Sage Publications},
  address	= {Newbury Park, CA},
  edition	= {},
  year  	= {1993}
}

***** END jsp added

%------ Applied Asymptotic Analysis

@book{appliedasymptotic,
	author = {Peter Miller},
	edition = {1},
	publisher= {American Mathematical Society},
	title = {Applied Asymptotic Analysis},
	year = {2006}}


%------ Bai Normal Beta Prime Prior

@article{BaiHypothesisNB,
author = {Ray Bai and Malay Ghosh},
title = {Large-scale multiple hypothesis testing with the normal-beta prime prior},
journal = {Statistics},
volume = {53},
number = {6},
pages = {1210-1233},
year  = {2019},
publisher = {Taylor & Francis},
doi = {10.1080/02331888.2019.1662017},

URL = {
        https://doi.org/10.1080/02331888.2019.1662017

},
eprint = {
        https://doi.org/10.1080/02331888.2019.1662017

}

}

%----- Barr

@article{Barr2013RandomES,
  title={Random effects structure for confirmatory hypothesis testing: Keep it maximal.},
  author={Dale J. Barr and R. Levy and Christoph Scheepers and Harry J. Tily},
  journal={Journal of memory and language},
  year={2013},
  volume={68 3}
}

%----- Bayesian Lasso
@article{bayesianlasso,
author = {Trevor Park and George Casella},
title = {The Bayesian Lasso},
journal = {Journal of the American Statistical Association},
volume = {103},
number = {482},
pages = {681-686},
year  = {2008},
publisher = {Taylor & Francis},
doi = {10.1198/016214508000000337},

URL = {
        https://doi.org/10.1198/016214508000000337

},
eprint = {
        https://doi.org/10.1198/016214508000000337

}

}

%----- Bayesian Penalized Regression
@article{BayesPenalizedRegSara,
title = "Shrinkage priors for Bayesian penalized regression",
abstract = "In linear regression problems with many predictors, penalized regression techniques are often used to guard against overfitting and to select variables relevant for predicting an outcome variable. Recently, Bayesian penalization is becoming increasingly popular in which the prior distribution performs a function similar to that of the penalty term in classical penalization. Specifically, the so-called shrinkage priors in Bayesian penalization aim to shrink small effects to zero while maintaining true large effects. Compared to classical penalization techniques, Bayesian penalization techniques perform similarly or sometimes even better, and they offer additional advantages such as readily available uncertainty estimates, automatic estimation of the penalty parameter, and more flexibility in terms of penalties that can be considered. However, many different shrinkage priors exist and the available, often quite technical, literature primarily focuses on presenting one shrinkage prior and often provides comparisons with only one or two other shrinkage priors. This can make it difficult for researchers to navigate through the many prior options and choose a shrinkage prior for the problem at hand. Therefore, the aim of this paper is to provide a comprehensive overview of the literature on Bayesian penalization. We provide a theoretical and conceptual comparison of nine different shrinkage priors and parametrize the priors, if possible, in terms of scale mixture of normal distributions to facilitate comparisons. We illustrate different characteristics and behaviors of the shrinkage priors and compare their performance in terms of prediction and variable selection in a simulation study. Additionally, we provide two empirical examples to illustrate the application of Bayesian penalization. Finally, an R package bayesreg is available online (https://github.com/sara-vanerp/bayesreg) which allows researchers to perform Bayesian penalized regression with novel shrinkage priors in an easy manner.",
keywords = "ADAPTIVE LASSO, Bayesian, Empirical Bayes, FREQUENTIST, HORSESHOE, INFORMATION, MODELS, Penalization, REGULARIZATION, Regression, Shrinkage priors, VARIABLE-SELECTION",
author = "{Van Erp}, Sara and Oberski, {Daniel L.} and Joris Mulder",
year = "2019",
doi = "10.1016/j.jmp.2018.12.004",
language = "English",
volume = "89",
pages = "31--50",
journal = "Journal of Mathematical Psychology",
issn = "0022-2496",
publisher = "ACADEMIC PRESS INC ELSEVIER SCIENCE",
}

%------- Bayesian Workflow

@misc{bayesianworkflow,
  doi = {10.48550/ARXIV.2011.01808},

  url = {https://arxiv.org/abs/2011.01808},

  author = {Gelman, Andrew and Vehtari, Aki and Simpson, Daniel and Margossian, Charles C. and Carpenter, Bob and Yao, Yuling and Kennedy, Lauren and Gabry, Jonah and Bürkner, Paul-Christian and Modrák, Martin},

  keywords = {Methodology (stat.ME), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {Bayesian Workflow},

  publisher = {arXiv},

  year = {2020},

  copyright = {Creative Commons Attribution 4.0 International}
}


%------ beta distribution mean and precision parametrization

@incollection{meanprecbeta,
title = {Chapter 6 - Inferring a Binomial Probability via Exact Mathematical Analysis},
editor = {John K. Kruschke},
booktitle = {Doing Bayesian Data Analysis (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Boston},
pages = {123-141},
year = {2015},
isbn = {978-0-12-405888-0},
doi = {https://doi.org/10.1016/B978-0-12-405888-0.00006-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780124058880000064},
author = {John K. Kruschke}
}

%----- Berger

@book{berger1985statistical,
  title={Statistical Decision Theory and Bayesian Analysis},
  author={Berger, J.O.},
  isbn={9780387960982},
  lccn={93175464},
  series={Springer Series in Statistics},
  url={https://books.google.de/books?id=oY\_x7dE15\_AC},
  year={1985},
  publisher={Springer}
}




%---- browne draper

@article{BrowneDraper,
author = {William J. Browne and David Draper},
title = {{A comparison of Bayesian and likelihood-based methods for fitting multilevel models}},
volume = {1},
journal = {Bayesian Analysis},
number = {3},
publisher = {International Society for Bayesian Analysis},
pages = {473 -- 514},
keywords = {adaptive MCMC, bias, Calibration, diffuse priors, hierarchical modeling, hybrid Metropolis-Gibbs sampling, IGLS, interval coverage, intraclass correlation, mixed models, MQL, PQL, random-effects logistic regression, REML, RIGLS, variance-components models},
year = {2006},
doi = {10.1214/06-BA117},
URL = {https://doi.org/10.1214/06-BA117}
}


%----- Buerkner Monotonic effects

@article{buerknermonotonic,
author = {Bürkner, Paul-Christian and Charpentier, Emmanuel},
title = {Modelling monotonic effects of ordinal predictors in Bayesian regression models},
journal = {British Journal of Mathematical and Statistical Psychology},
volume = {73},
number = {3},
pages = {420-451},
keywords = {Bayesian statistics, brms, isotonic regression, ordinal variables, Stan, R},
doi = {https://doi.org/10.1111/bmsp.12195},
url = {https://bpspsychub.onlinelibrary.wiley.com/doi/abs/10.1111/bmsp.12195},
eprint = {https://bpspsychub.onlinelibrary.wiley.com/doi/pdf/10.1111/bmsp.12195},
abstract = {Ordinal predictors are commonly used in regression models. They are often incorrectly treated as either nominal or metric, thus under- or overestimating the information contained. Such practices may lead to worse inference and predictions compared to methods which are specifically designed for this purpose. We propose a new method for modelling ordinal predictors that applies in situations in which it is reasonable to assume their effects to be monotonic. The parameterization of such monotonic effects is realized in terms of a scale parameter b representing the direction and size of the effect and a simplex parameter modelling the normalized differences between categories. This ensures that predictions increase or decrease monotonically, while changes between adjacent categories may vary across categories. This formulation generalizes to interaction terms as well as multilevel structures. Monotonic effects may be applied not only to ordinal predictors, but also to other discrete variables for which a monotonic relationship is plausible. In simulation studies we show that the model is well calibrated and, if there is monotonicity present, exhibits predictive performance similar to or even better than other approaches designed to handle ordinal predictors. Using Stan, we developed a Bayesian estimation method for monotonic effects which allows us to incorporate prior information and to check the assumption of monotonicity. We have implemented this method in the R package brms, so that fitting monotonic effects in a fully Bayesian framework is now straightforward.},
year = {2020}
}




%--- BRMS JSS

@article{brmsJSS,
 title={brms: An R Package for Bayesian Multilevel Models Using Stan},
 volume={80},
 url={https://www.jstatsoft.org/index.php/jss/article/view/v080i01},
 doi={10.18637/jss.v080.i01},
 abstract={The brms package implements Bayesian multilevel models in R using the probabilistic programming language Stan. A wide range of distributions and link functions are supported, allowing users to fit - among others - linear, robust linear, binomial, Poisson, survival, ordinal, zero-inflated, hurdle, and even non-linear models all in a multilevel context. Further modeling options include autocorrelation of the response variable, user defined covariance structures, censored data, as well as meta-analytic standard errors. Prior specifications are flexible and explicitly encourage users to apply prior distributions that actually reflect their beliefs. In addition, model fit can easily be assessed and compared with the Watanabe-Akaike information criterion and leave-one-out cross-validation.},
 number={1},
 journal={Journal of Statistical Software},
 author={Bürkner, Paul-Christian},
 year={2017},
 pages={1–28}
}


%------- BRMS items
@article{brmsJSSitems,
 title={Bayesian Item Response Modeling in R with brms and Stan},
 volume={100},
 url={https://www.jstatsoft.org/index.php/jss/article/view/v100i05},
 doi={10.18637/jss.v100.i05},
 abstract={&amp;lt;p&amp;gt;Item response theory (IRT) is widely applied in the human sciences to model persons’ responses on a set of items measuring one or more latent constructs. While several R packages have been developed that implement IRT models, they tend to be restricted to respective pre-specified classes of models. Further, most implementations are frequentist while the availability of Bayesian methods remains comparably limited. I demonstrate how to use the R package brms together with the probabilistic programming language Stan to specify and fit a wide range of Bayesian IRT models using flexible and intuitive multilevel formula syntax. Further, item and person parameters can be related in both a linear or non-linear manner. Various distributions for categorical, ordinal, and continuous responses are supported. Users may even define their own custom response distribution for use in the presented framework. Common IRT model classes that can be specified natively in the presented framework include 1PL and 2PL logistic models optionally also containing guessing parameters, graded response and partial credit ordinal models, as well as drift diffusion models of response times coupled with binary decisions. Posterior distributions of item and person parameters can be conveniently extracted and postprocessed. Model fit can be evaluated and compared using Bayes factors and efficient cross-validation procedures.&amp;lt;/p&amp;gt;},
 number={5},
 journal={Journal of Statistical Software},
 author={Bürkner, Paul-Christian},
 year={2021},
 pages={1–54}
}


%------------ Catalina Buerkner ProjPred MLM

@misc{catalina2020projection,
      title={Projection Predictive Inference for Generalized Linear and Additive Multilevel Models},
      author={Alejandro Catalina and Paul-Christian Bürkner and Aki Vehtari},
      year={2020},
      eprint={2010.06994},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}

%-------- Depaoli

@article{Depaoli2015ABA,
  title={A Bayesian Approach to Multilevel Structural Equation Modeling With Continuous and Dichotomous Outcomes},
  author={Sarah Depaoli and James P. Clifton},
  journal={Structural Equation Modeling: A Multidisciplinary Journal},
  year={2015},
  volume={22},
  pages={327 - 351}
}

%------ Dobson

@book{Dobson,
	author = {Dobson A.J and Barnett A.G},
	edition = {4},
	publisher = {Chapman and Hall/CRC},
	title = { An Introduction to Generalized Linear Models},
	year = {2018}}



%-------- Dirichlet Distribution Properties

@inproceedings{OnTheDirichlet,
  title={On The Dirichlet Distribution by Jiayu Lin},
  author={Jiayu Lin},
  year={2016}
}

%-------- Dirichlet Laplace

@article{DirichletLaplace,
author = {Anirban Bhattacharya and Debdeep Pati and Natesh S. Pillai and David B. Dunson},
title = {Dirichlet–Laplace Priors for Optimal Shrinkage},
journal = {Journal of the American Statistical Association},
volume = {110},
number = {512},
pages = {1479-1490},
year  = {2015},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.2014.960967},
    note ={PMID: 27019543},

URL = {
        https://doi.org/10.1080/01621459.2014.960967

},
eprint = {
        https://doi.org/10.1080/01621459.2014.960967

}

}



%-------- FDR Benjamini

@article{FDRBenjamini,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2346101},
 abstract = {The common approach to the multiplicity problem calls for controlling the familywise error rate (FWER). This approach, though, has faults, and we point out a few. A different approach to problems of multiple significance testing is presented. It calls for controlling the expected proportion of falsely rejected hypotheses-the false discovery rate. This error rate is equivalent to the FWER when all hypotheses are true but is smaller otherwise. Therefore, in problems where the control of the false discovery rate rather than that of the FWER is desired, there is potential for a gain in power. A simple sequential Bonferroni-type procedure is proved to control the false discovery rate for independent test statistics, and a simulation study shows that the gain in power is substantial. The use of the new procedure and the appropriateness of the criterion are illustrated with examples.},
 author = {Yoav Benjamini and Yosef Hochberg},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {1},
 pages = {289--300},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing},
 urldate = {2022-06-28},
 volume = {57},
 year = {1995}
}


%---- Fulgstad

@article{Fulgstad2019,
author = {Geir-Arne Fuglstad and Daniel Simpson and Finn Lindgren and Håvard Rue},
title = {Constructing Priors that Penalize the Complexity of Gaussian Random Fields},
journal = {Journal of the American Statistical Association},
volume = {114},
number = {525},
pages = {445-452},
year  = {2019},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.2017.1415907},

URL = {
        https://doi.org/10.1080/01621459.2017.1415907

},
eprint = {
        https://doi.org/10.1080/01621459.2017.1415907

}

}


%-------  Gelman

@article{GelmanWeaklyInf,
author = {Andrew Gelman and Aleks Jakulin and Maria Grazia Pittau and Yu-Sung Su},
title = {{A weakly informative default prior distribution for logistic and other regression models}},
volume = {2},
journal = {The Annals of Applied Statistics},
number = {4},
publisher = {Institute of Mathematical Statistics},
pages = {1360 -- 1383},
keywords = {Bayesian inference, generalized linear model, hierarchical model, least squares, Linear regression, logistic regression, multilevel model, noninformative prior distribution, weakly informative prior distribution},
year = {2008},
doi = {10.1214/08-AOAS191},
URL = {https://doi.org/10.1214/08-AOAS191}
}

%-------  Gelman
@article{Gelmanmulticomp,
author = { Andrew   Gelman  and  Jennifer   Hill  and  Masanao   Yajima },
title = {Why We (Usually) Don't Have to Worry About Multiple Comparisons},
journal = {Journal of Research on Educational Effectiveness},
volume = {5},
number = {2},
pages = {189-211},
year  = {2012},
publisher = {Routledge},
doi = {10.1080/19345747.2011.618213},

URL = {
        https://doi.org/10.1080/19345747.2011.618213

},
eprint = {
        https://doi.org/10.1080/19345747.2011.618213

}
}



%------ Gelman Hill

@book{gelman_hill_2006, place={Cambridge}, series={Analytical Methods for Social Research}, title={Data Analysis Using Regression and Multilevel/Hierarchical Models}, DOI={10.1017/CBO9780511790942}, publisher={Cambridge University Press}, author={Gelman, Andrew and Hill, Jennifer}, year={2006}, collection={Analytical Methods for Social Research}}

%------ Gelman BDA

@book{gelman2013bda,
  title={Bayesian Data Analysis, Third Edition},
  author={Gelman, A. and Carlin, J.B. and Stern, H.S. and Dunson, D.B. and Vehtari, A. and Rubin, D.B.},
  isbn={9781439840955},
  lccn={2013039507},
  series={Chapman \& Hall/CRC Texts in Statistical Science},
  url={https://books.google.de/books?id=ZXL6AQAAQBAJ},
  year={2013},
  publisher={Taylor \& Francis}
}

%----------- Gelman 2020


@book{gelmanregstories2020, place={Cambridge}, series={Analytical Methods for Social Research}, title={Regression and Other Stories}, DOI={10.1017/9781139161879}, publisher={Cambridge University Press}, author={Gelman, Andrew and Hill, Jennifer and Vehtari, Aki}, year={2020}, collection={Analytical Methods for Social Research}}

%------- Gelman half student t
@article{GelmanHalfStudentt,
author = {Andrew Gelman},
title = {{Prior distributions for variance parameters in hierarchical models (comment on article by Browne and Draper)}},
volume = {1},
journal = {Bayesian Analysis},
number = {3},
publisher = {International Society for Bayesian Analysis},
pages = {515 -- 534},
keywords = {Bayesian inference, conditional conjugacy, folded-noncentral-$t$ distribution, half-$t$ distribution, hierarchical model, multilevel model, noninformative prior distribution, weakly informative prior distribution},
year = {2006},
doi = {10.1214/06-BA117A},
URL = {https://doi.org/10.1214/06-BA117A}
}

%---------- Generalized inverse Gaussian

@article{GenInvGaussian,
title = {Generating generalized inverse Gaussian random variates by fast inversion},
journal = {Computational Statistics and Data Analysis},
volume = {55},
number = {1},
pages = {213-217},
year = {2011},
issn = {0167-9473},
doi = {https://doi.org/10.1016/j.csda.2010.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0167947310002847},
author = {Josef Leydold and Wolfgang Hörmann},
keywords = {Generalized inverse Gaussian distribution, Random variate generation, Numerical inversion},
abstract = {The inversion method for generating non-uniformly distributed random variates is a crucial part in many applications of Monte Carlo techniques, e.g., when low discrepancy sequences or copula based models are used. Unfortunately, closed form expressions of quantile functions of important distributions are often not available. The (generalized) inverse Gaussian distribution is a prominent example. It is shown that algorithms that are based on polynomial approximation are well suited for this distribution. Their precision is close to machine precision and they are much faster than root finding methods like the bisection method that has been recently proposed.}
}

@article{RobertGIG,
title = {Generalized inverse normal distributions},
journal = {Statistics \& Probability Letters},
volume = {11},
number = {1},
pages = {37-41},
year = {1991},
issn = {0167-7152},
doi = {https://doi.org/10.1016/0167-7152(91)90174-P},
url = {https://www.sciencedirect.com/science/article/pii/016771529190174P},
author = {Christian Robert},
keywords = {Inverse Gaussian, Bayesian estimation, conjugate prior, multiplicative model},
abstract = {In this paper, we study the basic properties of the generalized inverse normal distribution, natural alternative to the inverse Gaussian distribution, and we show that it generates a conjugate prior family in a normal estimation problem.}
}


%-----------Gibbs Samplers and Multigrid Decompositions


@article{GibbsMultigrid,
author = {Giacomo Zanella and Gareth Roberts},
title = {{Multilevel Linear Models, Gibbs Samplers and Multigrid Decompositions (with Discussion)}},
volume = {16},
journal = {Bayesian Analysis},
number = {4},
publisher = {International Society for Bayesian Analysis},
pages = {1309 -- 2770},
keywords = {centred and non-centred parametrizations, Convergence rates, Gibbs sampler, hierarchical models, multigrid decomposition, statistical identifiability},
year = {2021},
doi = {10.1214/20-BA1242},
URL = {https://doi.org/10.1214/20-BA1242}
}

%------ Goodrich rstanarm

@Misc{rstanarm,
  title = {rstanarm: {Bayesian} applied regression modeling via {Stan}.},
  author = {Ben Goodrich and Jonah Gabry and Imad Ali and Sam Brilleman},
  note = {R package version 2.21.1},
  year = {2020},
  url = {https://mc-stan.org/rstanarm},
}

%------ Handbook MCMC

@book{handbookmcmc,
	author = {Brooks and S. Gelman and A. Jones},
	date-added = {2022-05-18 11:38:47 +0200},
	date-modified = {2022-05-18 11:40:01 +0200},
	edition = {1},
	publisher = {Chapman and Hall/CRC},
	title = {Handbook of Markov Chain Monte Carlo},
	year = {2011}}

%------- Harrell

@book{harrell2013regression,
  title={Regression Modeling Strategies: With Applications to Linear Models, Logistic Regression, and Survival Analysis},
  author={Harrell, F.E.},
  isbn={9781475734621},
  lccn={2001020045},
  series={Springer Series in Statistics},
  url={https://books.google.de/books?id=7D0mBQAAQBAJ},
  year={2013},
  publisher={Springer New York}
}



%------- High Dimensional Stats with a view towards apps in Bio


@article{BuehlmannBio,
author = {Buehlmann, Peter and Kalisch, Markus and Meier, Lukas},
title = {High-Dimensional Statistics with a View Toward Applications in Biology},
journal = {Annual Review of Statistics and Its Application},
volume = {1},
number = {1},
pages = {255-278},
year = {2014},
doi = {10.1146/annurev-statistics-022513-115545},

URL = {
        https://doi.org/10.1146/annurev-statistics-022513-115545

},
eprint = {
        https://doi.org/10.1146/annurev-statistics-022513-115545

}
,
    abstract = { We review statistical methods for high-dimensional data analysis and pay particular attention to recent developments for assessing uncertainties in terms of controlling false positive statements (type I error) and p-values. The main focus is on regression models, but we also discuss graphical modeling and causal inference based on observational data. We illustrate the concepts and methods with various packages from the statistical software using a high-throughput genomic data set about riboflavin production with Bacillus subtilis, which we make publicly available for the first time. }
}

%------ Horseshoe
@article{Horseshoe,
 ISSN = {00063444, 14643510},
 URL = {http://www.jstor.org/stable/25734098},
 abstract = {This paper proposes a new approach to sparsity, called the horseshoe estimator, which arises from a prior based on multivariate-normal scale mixtures. We describe the estimator's advantages over existing approaches, including its robustness, adaptivity to different sparsity patterns and analytical tractability. We prove two theorems: one that characterizes the horseshoe estimator's tail robustness and the other that demonstrates a super-efficient rate of convergence to the correct estimate of the sampling density in sparse situations. Finally, using both real and simulated data, we show that the horseshoe estimator corresponds quite closely to the answers obtained by Bayesian model averaging under a point-mass mixture prior.},
 author = {Carvalho and Polson and Scott},
 journal = {Biometrika},
 number = {2},
 pages = {465--480},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {The horseshoe estimator for sparse signals},
 urldate = {2022-04-07},
 volume = {97},
 year = {2010}
}


@InProceedings{HorseshoeProceedings,
  title = 	 {Handling Sparsity via the Horseshoe},
  author = 	 {Carvalho, Carlos M. and Polson, Nicholas G. and Scott, James G.},
  booktitle = 	 {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {73--80},
  year = 	 {2009},
  editor = 	 {van Dyk, David and Welling, Max},
  volume = 	 {5},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
  month = 	 {16--18 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v5/carvalho09a/carvalho09a.pdf},
  url = 	 {https://proceedings.mlr.press/v5/carvalho09a.html},
  abstract = 	 {This paper presents a general, fully Bayesian framework for sparse supervised-learning problems based on the horseshoe prior. The horseshoe prior is a member of the family of multivariate scale mixtures of normals, and is therefore closely related to widely used approaches for sparse Bayesian learning, including, among others, Laplacian priors (e.g. the LASSO) and Student-t priors (e.g. the relevance vector machine). The advantages of the horseshoe are its robustness at handling unknown sparsity and large outlying signals. These properties are justifed theoretically via a representation theorem and accompanied by comprehensive empirical experiments that compare its performance to benchmark alternatives.}
}




%------ Horseshoe +

@article{Horseshoeplus,
author = {Anindya Bhadra and Jyotishka Datta and Nicholas G. Polson and Brandon Willard},
title = {{The Horseshoe+ Estimator of Ultra-Sparse Signals}},
volume = {12},
journal = {Bayesian Analysis},
number = {4},
publisher = {International Society for Bayesian Analysis},
pages = {1105 -- 1131},
keywords = {Bayesian, global–local shrinkage, horseshoe, horseshoe+, normal means, Sparsity},
year = {2017},
doi = {10.1214/16-BA1028},
URL = {https://doi.org/10.1214/16-BA1028}
}


%--------- Jeffreys

@article{Jeffreys,
  title={Theory of Probability Harold Jeffreys (Third edition, 447 + ix pp., Oxford Univ. Press, 84s.)},
  author={I. J. Good},
  journal={Geophysical Journal International},
  year={1962},
  volume={6},
  pages={555-558}
}


%------ Kruijer

@article{SimplexKruijer,
author = {Kruijer, Willem and Rousseau, Judith and Vaart, Aad},
year = {2010},
month = {01},
pages = {},
title = {Adaptive Bayesian Density Estimation with Location-Scale Mixtures},
volume = {4},
journal = {Electronic Journal of Statistics},
doi = {10.1214/10-EJS584}
}


%------- Kumaraswamy

@article{Kumaraswamy,
title = {Kumaraswamy’s distribution: A beta-type distribution with some tractability advantages},
journal = {Statistical Methodology},
volume = {6},
number = {1},
pages = {70-81},
year = {2009},
issn = {1572-3127},
doi = {https://doi.org/10.1016/j.stamet.2008.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1572312708000282},
author = {M.C. Jones},
keywords = {Beta distribution, Distribution theory, Minimax distribution, Minimum of maxima, Order statistics},
abstract = {A two-parameter family of distributions on (0,1) is explored which has many similarities to the beta distribution and a number of advantages in terms of tractability (it also, of course, has some disadvantages). Kumaraswamy’s distribution has its genesis in terms of uniform order statistics, and has particularly straightforward distribution and quantile functions which do not depend on special functions (and hence afford very easy random variate generation). The distribution might, therefore, have a particular role when a quantile-based approach to statistical modelling is taken, and its tractability has appeal for pedagogical uses. To date, the distribution has seen only limited use and development in the hydrological literature.}
}


%----- Laura Dietz

@article{LauraDietz,
    title={Directed Factor Graph Notation for Generative Models.},
    author={Dietz, Laura},
    url={https://github.com/jluttine/tikz-bayesnet#related-projects}
}

%------ Laura Dietz Bayes Net

@misc{ TikzBayesNet,
  author = { Laura Dietz, Jaakko Luttinen, Brendan O'connor},
  title = {{BayesNet}},
  year={2010},
  url = {https://github.com/jluttine/tikz-bayesnet}
}





%------ lme4

@article{lme4,
 title={Fitting Linear Mixed-Effects Models Using lme4},
 volume={67},
 url={https://www.jstatsoft.org/index.php/jss/article/view/v067i01},
 doi={10.18637/jss.v067.i01},
 abstract={Maximum likelihood or restricted maximum likelihood (REML) estimates of the parameters in linear mixed-effects models can be determined using the lmer function in the lme4 package for R. As for most model-fitting functions in R, the model is described in an lmer call by a formula, in this case including both fixed- and random-effects terms. The formula and data together determine a numerical representation of the model from which the profiled deviance or the profiled REML criterion can be evaluated as a function of some of the model parameters. The appropriate criterion is optimized, using one of the constrained optimization functions in R, to provide the parameter estimates. We describe the structure of the model, the steps in evaluating the profiled deviance or REML criterion, and the structure of classes or types that represents such a model. Sufficient detail is included to allow specialization of these structures by users who wish to write functions to fit specialized linear mixed models, such as models incorporating pedigrees or smoothing splines, that are not easily expressible in the formula language used by lmer.},
 number={1},
 journal={Journal of Statistical Software},
 author={Bates, Douglas and Mächler, Martin and Bolker, Ben and Walker, Steve},
 year={2015},
 pages={1–48}
}


%-------- Logistic normal

@article{logisticnormal,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2335470},
 abstract = {The logistic transformation applied to a d-dimensional normal distribution produces a distribution over the d-dimensional simplex which can sensibly be termed a logistic-normal distribution. Such distributions, implicitly used in a number of recent applications, are here given a formal identity and some useful properties are recorded. A main aim is to extend the area of application from the restricted role as a substitute for the Dirichlet conjugate prior class in the analysis of multinomial and contingency table data to the direct statistical description and analysis of compositional and probabilistic data.},
 author = {J. Aitchison and S. M. Shen},
 journal = {Biometrika},
 number = {2},
 pages = {261--272},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Logistic-Normal Distributions: Some Properties and Uses},
 urldate = {2022-07-28},
 volume = {67},
 year = {1980}
}

%--------- Lu Xia
% riboflavin dataset is used here too

@misc{highdimgees,
  doi = {10.48550/ARXIV.2207.11686},

  url = {https://arxiv.org/abs/2207.11686},

  author = {Xia, Lu and Shojaie, Ali},

  keywords = {Methodology (stat.ME), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {Statistical inference for high-dimensional generalized estimating equations},

  publisher = {arXiv},

  year = {2022},

  copyright = {Creative Commons Attribution 4.0 International}
}


%------- Morris

@article{Morris2019BayesianHS,
  title={Bayesian hierarchical spatial models: Implementing the Besag York Molli{\'e} model in stan.},
  author={Mitzi Morris and Katherine Wheeler-Martin and Daniel P. Simpson and Stephen J. Mooney and Andrew Gelman and Charles J DiMaggio},
  journal={Spatial and spatio-temporal epidemiology},
  year={2019},
  volume={31},
  pages={
          100301
        }
}

%------ Nist Handbook

@book{nisthandbook,
  title={NIST Handbook of Mathematical Functions Hardback and CD-ROM},
  author={Olver, F.W.J. and National Institute of Standards and Technology (U.S.) and Lozier, D.W. and Boisvert, R.F. and Clark, C.W.},
  isbn={9780521192255},
  lccn={2010281142},
  url={https://books.google.de/books?id=3I15Ph1Qf38C},
  year={2010},
  publisher={Cambridge University Press}
}

%------- Nishimura Shrinking Shoulders

@article{ShrinkingShoulders,
author = {Akihiko Nishimura and Marc A. Suchard},
title = {{Shrinkage with Shrunken Shoulders: Gibbs Sampling Shrinkage Model Posteriors with Guaranteed Convergence Rates}},
journal = {Bayesian Analysis},
publisher = {International Society for Bayesian Analysis},
pages = {1 -- 24},
keywords = {Bayesian inference, ergodicity, generalized linear model, Markov chain Monte Carlo, Sparsity},
year = {2022},
doi = {10.1214/22-BA1308},
URL = {https://doi.org/10.1214/22-BA1308}
}



%-------- No U turn sampler

@article{nuts,
author = {Hoffman, Matthew D. and Gelman, Andrew},
title = {The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo},
year = {2014},
publisher = {JMLR.org},
volume = {15},
number = {1},
issn = {1532-4435},
abstract = {Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by first-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMC's performance is highly sensitive to two user-specified parameters: a step size ε and a desired number of steps L. In particular, if L is too small then the algorithm exhibits undesirable random walk behavior, while if L is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps L. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS performs at least as efficiently as (and sometimes more effciently than) a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter ε on the fly based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all, making it suitable for applications such as BUGS-style automatic inference engines that require efficient "turnkey" samplers.},
journal = {J. Mach. Learn. Res.},
pages = {1593–1623},
numpages = {31},
keywords = {Hamiltonian Monte Carlo, Bayesian inference, Markov chain Monte Carlo, adaptive Monte Carlo, dual averaging}
}


%------- Penalized complexity priors
@article{pcpriors,
author = {Daniel Simpson and Håvard Rue and Andrea Riebler and Thiago G. Martins and Sigrunn H. Sørbye},
title = {{Penalising Model Component Complexity: A Principled, Practical Approach to Constructing Priors}},
volume = {32},
journal = {Statistical Science},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {1 -- 28},
keywords = {Bayesian theory, disease mapping, hierarchical models, information geometry, interpretable prior distributions, prior on correlation matrices},
year = {2017},
doi = {10.1214/16-STS576},
URL = {https://doi.org/10.1214/16-STS576}
}



%------- Piironen Aki Regularized Horseshoe

@article{PiironenHorseshoe,
author = {Juho Piironen and Aki Vehtari},
title = {{Sparsity information and regularization in the horseshoe and other shrinkage priors}},
volume = {11},
journal = {Electronic Journal of Statistics},
number = {2},
publisher = {Institute of Mathematical Statistics and Bernoulli Society},
pages = {5018 -- 5051},
keywords = {Bayesian inference, horseshoe prior, shrinkage priors, Sparse estimation},
year = {2017},
doi = {10.1214/17-EJS1337SI},
URL = {https://doi.org/10.1214/17-EJS1337SI}
}



%------ Piironen projective inference

@article{PiironenProjInf,
author = {Juho Piironen and Markus Paasiniemi and Aki Vehtari},
title = {{Projective inference in high-dimensional problems: Prediction and feature selection}},
volume = {14},
journal = {Electronic Journal of Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics and Bernoulli Society},
pages = {2155 -- 2197},
keywords = {Feature selection, Post-selection inference, prediction, projection, Sparsity},
year = {2020},
doi = {10.1214/20-EJS1711},
URL = {https://doi.org/10.1214/20-EJS1711}
}



%------ Polson Scott
@article{PolsonHalfCauchy,
author = {Nicholas G. Polson and James G. Scott},
title = {{On the Half-Cauchy Prior for a Global Scale Parameter}},
volume = {7},
journal = {Bayesian Analysis},
number = {4},
publisher = {International Society for Bayesian Analysis},
pages = {887 -- 902},
keywords = {hierarchical models, normal scale mixtures, shrinkage},
year = {2012},
doi = {10.1214/12-BA730},
URL = {https://doi.org/10.1214/12-BA730}
}





@article{PolsonDefault,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/26363497},
 abstract = {We provide a framework for assessing the default nature of a prior distribution using the property of regular variation, which we study for global-local shrinkage priors. In particular, we show that the horseshoe priors, originally designed to handle sparsity, are regularly varying and thus are appropriate for default Bayesian analysis. To illustrate our methodology, we discuss four problems of noninformative priors that have been shown to be highly informative for nonlinear functions. In each case, we show that global-local horseshoe priors perform as required. Global-local shrinkage priors can separate a low-dimensional signal from high-dimensional noise even for nonlinear functions.},
 author = {Anindya Bhadra and Jyotishka Datta and Nicholas G. Polson and Brandon Willard},
 journal = {Biometrika},
 number = {4},
 pages = {955--969},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Default Bayesian analysis with global-local shrinkage priors},
 urldate = {2022-07-28},
 volume = {103},
 year = {2016}
}


%---- Practical Bayesian

@article{elpdaki,
	author = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
	da = {2017/09/01},
	date-added = {2022-06-23 16:30:39 +0200},
	date-modified = {2022-06-23 16:31:15 +0200},
	doi = {10.1007/s11222-016-9709-3},
	id = {Vehtari2017},
	isbn = {1573-1375},
	journal = {Statistics and Computing},
	number = {5},
	pages = {1433--1433},
	title = {Erratum to: Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC},
	ty = {JOUR},
	url = {https://doi.org/10.1007/s11222-016-9709-3},
	volume = {27},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1007/s11222-016-9709-3}}


%---- Prior elicitation
@misc{prioreli,
      title={Prior knowledge elicitation: The past, present, and future},
      author={Petrus Mikkola and Osvaldo A. Martin and Suyog Chandramouli and Marcelo Hartmann and Oriol Abril Pla and Owen Thomas and Henri Pesonen and Jukka Corander and Aki Vehtari and Samuel Kaski and Paul-Christian Bürkner and Arto Klami},
      year={2021},
      eprint={2112.01380},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}

%------ Prob programming

@article{ProbProgrammingGorinova,
author = {Gorinova, Maria I. and Gordon, Andrew D. and Sutton, Charles},
title = {Probabilistic Programming with Densities in SlicStan: Efficient, Flexible, and Deterministic},
year = {2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {POPL},
url = {https://doi.org/10.1145/3290348},
doi = {10.1145/3290348},
abstract = {Stan is a probabilistic programming language that has been increasingly used for real-world scalable projects. However, to make practical inference possible, the language sacrifices some of its usability by adopting a block syntax, which lacks compositionality and flexible user-defined functions. Moreover, the semantics of the language has been mainly given in terms of intuition about implementation, and has not been formalised.  This paper provides a formal treatment of the Stan language, and introduces the probabilistic programming language SlicStan --- a compositional, self-optimising version of Stan. Our main contributions are (1) the formalisation of a core subset of Stan through an operational density-based semantics; (2) the design and semantics of the Stan-like language SlicStan, which facilities better code reuse and abstraction through its compositional syntax, more flexible functions, and information-flow type system; and (3) a formal, semantic-preserving procedure for translating SlicStan to Stan.},
journal = {Proc. ACM Program. Lang.},
month = {01},
articleno = {35},
numpages = {30},
keywords = {probabilistic programming, information flow analysis}
}

%------ Ridge regression

@article{Ridge,
 ISSN = {00401706},
 URL = {http://www.jstor.org/stable/1267351},
 abstract = {In multiple regression it is shown that parameter estimates based on minimum residual sum of squares have a high probability of being unsatisfactory, if not incorrect, if the prediction vectors are not orthogonal. Proposed is an estimation procedure based on adding small positive quantities to the diagonal of X′X. Introduced is the ridge trace, a method for showing in two dimensions the effects of nonorthogonality. It is then shown how to augment X′X to obtain biased estimates with smaller mean square error.},
 author = {Arthur E. Hoerl and Robert W. Kennard},
 journal = {Technometrics},
 number = {1},
 pages = {55--67},
 publisher = {[Taylor & Francis, Ltd., American Statistical Association, American Society for Quality]},
 title = {Ridge Regression: Biased Estimation for Nonorthogonal Problems},
 urldate = {2022-04-05},
 volume = {12},
 year = {1970}
}

%----- R2D2
@article{r2d2zhang,
author = {Yan Dora Zhang and Brian P. Naughton and Howard D. Bondell and Brian J. Reich},
title = {Bayesian Regression Using a Prior on the Model Fit: The R2-D2 Shrinkage Prior},
journal = {Journal of the American Statistical Association},
volume = {0},
number = {0},
pages = {1-13},
year  = {2020},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.2020.1825449},

URL = {
        https://doi.org/10.1080/01621459.2020.1825449
},
eprint = {
        https://doi.org/10.1080/01621459.2020.1825449

}

}

%------------- R2 MLM

@article{r2mlmNakagawa,
author = {Nakagawa, Shinichi and Schielzeth, Holger},
title = {A general and simple method for obtaining R2 from generalized linear mixed-effects models},
journal = {Methods in Ecology and Evolution},
volume = {4},
number = {2},
pages = {133-142},
keywords = {coefficient of determination, goodness-of-fit, heritability, information criteria, intra-class correlation, linear models, model fit, repeatability, variance explained},
doi = {https://doi.org/10.1111/j.2041-210x.2012.00261.x},
url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/j.2041-210x.2012.00261.x},
eprint = {https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2041-210x.2012.00261.x},
abstract = {Summary The use of both linear and generalized linear mixed-effects models (LMMs and GLMMs) has become popular not only in social and medical sciences, but also in biological sciences, especially in the field of ecology and evolution. Information criteria, such as Akaike Information Criterion (AIC), are usually presented as model comparison tools for mixed-effects models. The presentation of ‘variance explained’ (R2) as a relevant summarizing statistic of mixed-effects models, however, is rare, even though R2 is routinely reported for linear models (LMs) and also generalized linear models (GLMs). R2 has the extremely useful property of providing an absolute value for the goodness-of-fit of a model, which cannot be given by the information criteria. As a summary statistic that describes the amount of variance explained, R2 can also be a quantity of biological interest. One reason for the under-appreciation of R2 for mixed-effects models lies in the fact that R2 can be defined in a number of ways. Furthermore, most definitions of R2 for mixed-effects have theoretical problems (e.g. decreased or negative R2 values in larger models) and/or their use is hindered by practical difficulties (e.g. implementation). Here, we make a case for the importance of reporting R2 for mixed-effects models. We first provide the common definitions of R2 for LMs and GLMs and discuss the key problems associated with calculating R2 for mixed-effects models. We then recommend a general and simple method for calculating two types of R2 (marginal and conditional R2) for both LMMs and GLMMs, which are less susceptible to common problems. This method is illustrated by examples and can be widely employed by researchers in any fields of research, regardless of software packages used for fitting mixed-effects models. The proposed method has the potential to facilitate the presentation of R2 for a wide range of circumstances.},
year = {2013}
}


%--------------- R2mlm sterba

@article{r2mlmssterba,
  title={Quantifying explained variance in multilevel models: An integrative framework for defining R-squared measures.},
  author={Jason D Rights and Sonya Kourany Sterba},
  journal={Psychological methods},
  year={2019},
  volume={24 3},
  pages={
          309-338
        }
}

%------------

%--------------- ROC curves

@article{roccurves,
title = {An introduction to ROC analysis},
journal = {Pattern Recognition Letters},
volume = {27},
number = {8},
pages = {861-874},
year = {2006},
note = {ROC Analysis in Pattern Recognition},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2005.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S016786550500303X},
author = {Tom Fawcett},
keywords = {ROC analysis, Classifier evaluation, Evaluation metrics},
abstract = {Receiver operating characteristics (ROC) graphs are useful for organizing classifiers and visualizing their performance. ROC graphs are commonly used in medical decision making, and in recent years have been used increasingly in machine learning and data mining research. Although ROC graphs are apparently simple, there are some common misconceptions and pitfalls when using them in practice. The purpose of this article is to serve as an introduction to ROC graphs and as a guide for using them in research.}
}


%-------------- Simulation Based Calibration

@misc{taltssbc,
      title={Validating Bayesian Inference Algorithms with Simulation-Based Calibration},
      author={Sean Talts and Michael Betancourt and Daniel Simpson and Aki Vehtari and Andrew Gelman},
      year={2020},
      eprint={1804.06788},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}


%-------------- Simulation Based Calibration Manual
@Manual{sbcmanual,
  title = {SBC: Simulation Based Calibration for rstan/cmdstanr models},
  author = {Shinyoung Kim and Hyunji Moon and Martin Modrák and Teemu Säilynoja},
  year = {2022},
  note = {https://hyunjimoon.github.io/SBC/, https://github.com/hyunjimoon/SBC/},
}


%------------ Stan Bob Carpenter JSS

@article{StanJSS,
 title={Stan: A Probabilistic Programming Language},
 volume={76},
 url={https://www.jstatsoft.org/index.php/jss/article/view/v076i01},
 doi={10.18637/jss.v076.i01},
 abstract={Stan is a probabilistic programming language for specifying statistical models. A Stan program imperatively defines a log probability function over parameters conditioned on specified data and constants. As of version 2.14.0, Stan provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized maximum likelihood estimates are calculated using optimization methods such as the limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm. Stan is also a platform for computing log densities and their gradients and Hessians, which can be used in alternative algorithms such as variational Bayes, expectation propagation, and marginal inference using approximate integration. To this end, Stan is set up so that the densities, gradients, and Hessians, along with intermediate quantities of the algorithm such as acceptance probabilities, are easily accessible. Stan can be called from the command line using the cmdstan package, through R using the rstan package, and through Python using the pystan package. All three interfaces support sampling and optimization-based inference with diagnostics and posterior analysis. rstan and pystan also provide access to log probabilities, gradients, Hessians, parameter transforms, and specialized plotting.},
 number={1},
 journal={Journal of Statistical Software},
 author={Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D. and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
 year={2017},
 pages={1–32}
}

Stan Development Team. YEAR. Stan Modeling Language Users Guide and Reference Manual, VERSION. https://mc-stan.org


%----------- Stan Manual

@Misc{stan2022,
    title = {Stan Modeling Language Users Guide and Reference Manual, Version 2.30.0},
    author = {{Stan Development Team}},
    year = {2022},
    url = {http://mc-stan.org/},
  }

%--------------

%--------------- Stats significance in High Dimensional LMMS

@inproceedings{LinLina,
author = {Lin, Lina and Drton, Mathias and Shojaie, Ali},
title = {Statistical Significance in High-Dimensional Linear Mixed Models},
year = {2020},
isbn = {9781450381031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412815.3416883},
doi = {10.1145/3412815.3416883},
abstract = {This paper develops an inferential framework for high-dimensional linear mixed effect models. Such models are suitable, e.g., when collecting n repeated measurements for M subjects. We consider a scenario where the number of fixed effects p is large (and may be larger than M), but the number of random effects q is small. Our framework is inspired by a recent line of work that proposes de-biasing penalized estimators to perform inference for high-dimensional linear models with fixed effects only. In particular, we demonstrate how to correct a 'naive' ridge estimator to build asymptotically valid confidence intervals for mixed effect models. We validate our theoretical results with numerical experiments that show that our method can successfully account for the correlation induced by the random effects. For a practical demonstration we consider a riboflavin production dataset that exhibits group structure, and show that conclusions drawn using our method are consistent with those obtained on a similar dataset without group structure.},
booktitle = {Proceedings of the 2020 ACM-IMS on Foundations of Data Science Conference},
pages = {171–181},
numpages = {11},
keywords = {confidence intervals, linear mixed effect model, variance component estimation, high-dimensional statistics},
location = {Virtual Event, USA},
series = {FODS '20}
}


%--------------- Teemu Graphical Test

@misc{teemubuerknergraphical,
      title={Graphical Test for Discrete Uniformity and its Applications in Goodness of Fit Evaluation and Multiple Sample Comparison},
      author={Teemu Säilynoja and Paul-Christian Bürkner and Aki Vehtari},
      year={2021},
      eprint={2103.10522},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}



%----- Topi Paananen

@article{Paananen2020GroupHA,
  title={Group Heterogeneity Assessment for Multilevel Models},
  author={Topi Paananen and Alejandro Catalina and Paul-Christian Burkner and Aki Vehtari},
  journal={arXiv: Methodology},
  year={2020}
}


%---------- Vehtari Aki

@article{AkiSurvey,
author = {Aki Vehtari and Janne Ojanen},
title = {{A survey of Bayesian predictive methods for model assessment, selection and comparison}},
volume = {6},
journal = {Statistics Surveys},
number = {none},
publisher = {Amer. Statist. Assoc., the Bernoulli Soc., the Inst. Math. Statist., and the Statist. Soc. Canada},
pages = {142 -- 228},
keywords = {Bayesian, cross-validation, decision theory, Expected utility, information criteria, model assessment, Model selection, predictive},
year = {2012},
doi = {10.1214/12-SS102},
URL = {https://doi.org/10.1214/12-SS102}
}


%------------- Wakefield Bayesian and Frequentist Regression Methods

@book{wakefield2013bayesian,
  title={Bayesian and Frequentist Regression Methods},
  author={Wakefield, J.},
  isbn={9781441909251},
  series={Springer Series in Statistics},
  url={https://books.google.de/books?id=OUJEAAAAQBAJ},
  year={2013},
  publisher={Springer New York}
}

%------- Wasserstein

@article{wasserstein,
author = {Ronald L. Wasserstein and Nicole A. Lazar},
title = {The ASA Statement on p-Values: Context, Process, and Purpose},
journal = {The American Statistician},
volume = {70},
number = {2},
pages = {129-133},
year  = {2016},
publisher = {Taylor & Francis},
doi = {10.1080/00031305.2016.1154108},

URL = {
        https://doi.org/10.1080/00031305.2016.1154108

},
eprint = {
        https://doi.org/10.1080/00031305.2016.1154108

}

}





%-------- Wood


@book{wood2017generalized,
  title={Generalized Additive Models: An Introduction with R},
  author={Wood, S.N.},
  isbn={9781498728331},
  lccn={2016048785},
  series={Chapman \& Hall / CRC texts in statistical science},
  url={https://books.google.de/books?id=OitmjwEACAAJ},
  year={2017},
  publisher={CRC Press/Taylor \& Francis Group}
}

@MISC{Wood2011mgcv,
title = "Fast stable restricted maximum likelihood and marginal likelihood
estimation of semiparametric generalized linear models",
author = "Wood, S N",
journal = "Journal of the Royal Statistical Society (B)",
volume = 73,
number = 1,
pages = "3--36",
year = 2011
}

%-------- Zwillinger

@incollection{Zwillinger,
title = {8–9 - Special Functions},
author = {Alan Jeffrey and Daniel Zwillinger and I.S. Gradshteyn and I.M. Ryzhik},
booktitle = {Table of Integrals, Series, and Products (Seventh Edition)},
publisher = {Academic Press},
edition = {Seventh Edition},
address = {Boston},
pages = {859-1048},
year = {2007},
isbn = {978-0-12-373637-6},
doi = {https://doi.org/10.1016/B978-0-08-047111-2.50016-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780080471112500169}
}


%----- Yanchenko

@misc{Yanchenko,
  doi = {10.48550/ARXIV.2111.10718},

  url = {https://arxiv.org/abs/2111.10718},

  author = {Yanchenko, Eric and Bondell, Howard D. and Reich, Brian J.},

  keywords = {Methodology (stat.ME), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {The R2D2 Prior for Generalized Linear Mixed Models},

  publisher = {arXiv},

  year = {2021},

  copyright = {arXiv.org perpetual, non-exclusive license}
}


%------ van der pas

@incollection{vanDP2021theoretical,
  title={Theoretical guarantees for the horseshoe and other global-local shrinkage priors},
  author={van der Pas, St{\'e}phanie},
  booktitle={Handbook of Bayesian Variable Selection},
  pages={133--160},
  year={2021},
  publisher={Chapman and Hall/CRC}
}
