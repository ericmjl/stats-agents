\section{Introduction}
Regression models are ubiquitous in the quantitative sciences and industry, making up a big part of all statistical data analyses. Their success can be explained by a combination of several factors, involving the ease of interpretation of their additive structure, rich mathematical theory, and (relative) simplicity of their estimation, to name a few key aspects \citep{gelmanregstories2020, harrell2013regression}. Despite these advantages, the vast amount of modeling options and required analyst choices render building interpretable, robust, and well-predicting regression models a highly difficult task.

As the complexity of the analyzed data increases, so does the required complexity of the applied regression models and we are bound to encounter multilevel structures which contain overall (population-level or fixed effects) and varying (group-specific, group-level, or random effects) terms. 	Overall effects influence every member of the population equally, whereas varying effects do so per group. For example, multilevel structures can be found in Psychology, Medicine, or Biology, due to the natural groupings of individuals, or repeated measurement of the same individuals in experiments or longitudinal studies. Multilevel models are designed specifically to account for the nested structure in multilevel data and are a widely applied class of regression models \citep{lme4, gelman_hill_2006, brmsJSSitems}. Their key statistical idea is to assume that a set of regression coefficients, defined according to the multilevel structure, originate from the same underlying distribution whose hyperparameters are subsequently estimated from the data. The implied partial-pooling property  increases the robustness of the parameter estimates, helps with uncertainty calibration, and improves out-of- sample predictive performance \citep{gelman2013bda, gelman_hill_2006}. While multilevel models can be estimated in both frequentist and Bayesian frameworks \citep{lme4, brmsJSS}, we will focus on the latter framework in this work, since it offers considerable flexibility in the specification and regularization of multilevel models \citep{gelman2013bda}.

From a Bayesian perspective, the above mentioned distributions of the regression coefficients constitute prior distributions (priors) that describe the uncertainty in the model parameters before seeing the data. The desirable properties of multilevel models can then be explained by the fact that these form joint priors over a set of parameters with shared hyperparameters, rather than separate independent priors for each parameter \citep{gelman2013bda}. Joint priors are a Bayesian success story more generally. For example, joint priors can help to improve the predictive performance of individual additive terms parameterized by more than one parameter, for example, splines, spatial, or monotonic effects \citep{buerknermonotonic, wood2017generalized, Morris2019BayesianHS}. With very few exceptions \citep{Fulgstad2019,rstanarm,Yanchenko}, the development of joint priors for Bayesian multilevel models has been limited to individual additive terms. In contrast, different terms, corresponding to different parameter sets, still receive mutually independent priors, for example, independent (inverse-)gamma, uniform, or half Student-$t$ priors for variance or standard deviation parameters  \citep{BrowneDraper,brmsJSS, pcpriors,Depaoli2015ABA}. As more and more terms are being added to the model while the number of observations remains constant, such models will overfit the data, leading to unreliable or uninterpretable estimates as well as bad out-of-sample predictions \citep{BayesPenalizedRegSara}. Also, overall Type I error rates may be severely inflated if many terms with mutually independent priors are tested \citep{gelman2013bda}.

Without advanced regularization methods such as joint priors, reliable and interpretable estimation of the required highly parameterized models is very hard to achieve. This is particularly obvious in the high dimensional case regime (more covariates than observations: $p>N$), where the number of model parameters $p$ becomes larger than the number of observations $N$ in the data and unregularized regression models would not be estimable \citep{Ridge}; but even if $p<N$, regression models often benefit strongly from regularization, for example, to prevent overfitting and Type I error inflation \citep{Ridge, gelman2013bda, BayesPenalizedRegSara}. While all of these difficulties apply to standard regression models already, they become even worse for multilevel models as the number of additive terms to explore further increases when considering all valid combinations of predictor coefficients and grouping factors which are permitted by the given data structure \citep{catalina2020projection, Barr2013RandomES, Paananen2020GroupHA}.

In the context of \textit{single-level} linear models, that is, linear models without multilevel structure, a wide range of joint shrinkage priors have been developed \citep{bayesianlasso, Horseshoe, PiironenHorseshoe, DirichletLaplace}. These models enable the estimation, interpretation, and selection of predictor terms and prevent overfitting even when the number of predictors is large and/or the data is small \citep{BayesPenalizedRegSara, vanDP2021theoretical}. On an abstract level, these priors can all be described as Global-Local (GL) priors because they have both, global parameters controlling the shrinkage of all terms jointly and local parameters controlling the (relative) shrinkage of individual terms \citep{vanDP2021theoretical}. Usually, they are combined with a location-scale distribution, such as normal or a double exponential, where the location is fixed to zero and the scale is computed as a product of global and local parameters. While these priors have been very successful within their supported model class \citep{BayesPenalizedRegSara, vanDP2021theoretical}, they remain limited in several key aspects which we aim to address in our work:

\begin{enumerate}
    \item Most existing joint priors for regression models require the manual specification of hyperparameters with potentially strong impact on the obtained inference \citep{BayesPenalizedRegSara}. These hyperparameters are often unintuitive and abstract especially for non-statistical experts, making these priors’ practical application and communication much more difficult.
    \item High dimensionality in multilevel models has so far been studied in cases where the number of overall coefficients $p$ is big while the amount of varying terms $q$ is very small, $p > N, q \ll N$. For example, in their papers, \cite{BuehlmannBio} and \cite{LinLina} carried out theoretical research and simulation studies of frequentist multilevel models with large $p$ yet $q \leq 2$.
    \item Existing joint priors on multilevel models focus only on the group-specific coefficients \citep{Fulgstad2019, rstanarm},  without at the same time considering the overall coefficients.
    Further, it remains unclear how these priors scale with the number of predictors when their coefficients vary across an increasing number of grouping factors. However, in order to ensure consistent and robust regularization in multilevel models, we need to define a single global joint prior on all coefficients, and ensure it scales well to large numbers of additive terms and complex multilevel structure.
\end{enumerate}

An initial step towards addressing these issues, at least for single-level linear models, is the R2D2 prior developed by \cite{r2d2zhang}. The main idea is to specify a prior on the coefficient of determination $R^2$ – also known as \textit{proportion of explained variance} – and decompose the \textit{explained variance} into individual variance components via Dirichlet Decomposition (abbreviated as D2), thus propagating uncertainty from $R^2$ on to the regression coefficients. We further motivate the use of priors over $R^2$ in Section \ref{sec:r2d2m2prior}. The R2D2 prior shares several desirable properties with other global-local shrinkage priors for single-level linear models  \citep{PolsonDefault, Horseshoe, r2d2zhang} and is easy to understand even for non-expert analysts since the only hyperparameters it requires are the shape parameters of the prior on $R^2$ and a single concentration parameter for the Dirichlet distribution. However, its scope is limited to single-level linear models. These simple models do not live up to the requirements of model-based research, where multilevel data structures are omnipresent, so a much more general solution is needed.

Therefore, the aim of our paper is to fill this gap by generalizing the R2D2 prior to models with increased complexity in terms of multilevel structures.  We name our new prior the \textit{R2D2M2 prior}, where M2 stands for \emph{M}ultilevel \emph{M}odels.
In parallel to our work, related ideas were put forward by \cite{Yanchenko} although with a different focus: they concentrate on defining useful $R^2$ measures for non normal likelihoods without studying the theoretical properties on the resulting priors as we do here.
\subsection{Main contributions}

Our main contributions are as follows:

\begin{itemize}
    \item We propose a prior over a global $R^2$ that jointly regularizes both the overall and varying coefficients at the same time. Our new prior's hyperparameters are easier to interpret, which facilitates their specification by the user.
    \item We expand upon the theoretical properties of the original R2D2 prior by considering normal base for distributions for the coefficients. We study concentration properties around the origin and behavior in the tails and show how both are dependent on the hyperparameters selected by the user.
    \item We propose the use of shrinkage factors to evaluate how the chosen hyperparameters determine the amount of shrinkage for each individual coefficient. We also propose a global measure of shrinkage, which quantifies the effective total amount of non-zero coefficients in the model. Both of these quantities were not considered for the R2D2 prior before.
    \item We implement both the R2D2 prior and the R2D2M2 prior in the probabilistic programming language Stan \citep{stan2022,StanJSS} and demonstrate that inference for our implementations is well calibrated. We also provide an implementation in the \texttt{brms} (version 2.19.2) \citep{brmsJSS} R package, which provides a high level interface to fit Bayesian generalized (non-)linear multivariate multilevel models using Stan. We further perform extensive simulations to test the capabilities of the R2D2M2 prior and discuss its shrinkage behavior, out-of-sample predictive performance, as well as the relationship of the latter with the amount of shrinkage. We also show that frequentist error metrics such as Type I and Type II errors can be controlled with the help of our prior even in complex scenarios.
    \item Our joint prior offers the possibility to study cases in which both the number of overall coefficients $p$ and varying terms $q$ are large and potentially both greater than $N$, that is, high dimensionality in both the overall and varying coefficients. It is to the best of our knowledge the first method that provides this. To demonstrate the prior's applicability and usefulness in such high-dimensional cases not only in simulations but also in the real world, we reanalyse the riboflavin dataset provided by \cite{BuehlmannBio}.

\end{itemize}

The remainder of the paper is organized as follows: Section \ref{sec:r2d2m2prior} starts by motivating the use of priors over $R^2$ (and thus the use of the R2D2M2 prior) by showing how weakly informative priors on the coefficients can translate into very informative priors for $R^2$. The construction of the R2D2M2 prior is presented along with insights that lead to better understanding of the prior. Section \ref{sec:Properties} presents theoretical properties of both the R2D2 and R2D2M2 prior such as marginal distributions and concentration properties as well as guidelines for hyperparameter selection. We introduce the concept of shrinkage factors for both the R2D2 and R2D2M2 prior as well as the relationship the prior's hyperparameters have with the effective number of nonzero coefficients in the model, thus extending the idea of \cite{PiironenHorseshoe} to multilevel models. This provides an easy and intuitive way of setting up the prior, since it is possible to visualize how much sparsity will be induced. Section \ref{sec:examples} shows the results of testing the R2D2M2 prior in intensive simulations. We make use of simulation based calibration \citep{taltssbc} and provide evidence that that our implementation and estimation of both the R2D2 and the R2D2M2 prior models are well calibrated. What is more, we simulate data from sparse multilevel models and provide detailed discussions about how the prior behaves with respect to estimation error, out-of-sample predictive performance, credible interval coverage and Type I and Type II errors. Finally, we show how the R2D2M2 prior performs in real life data by testing it on the Riboflavin production data set made publicly available by \cite{BuehlmannBio}.
The paper ends with a discussion in Section \ref{sec:discussion} where we summarize our findings and suggest future lines of research. All mathematical proofs can be found in  Appendix \ref{appendixA}.
